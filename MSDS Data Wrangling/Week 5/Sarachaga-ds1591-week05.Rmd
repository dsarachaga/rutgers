---
title: "Homework 5"
author: "Diego Sar√°chaga"
date: "March 6, 2019"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}

library(tidyverse)
library(lubridate)
library(magrittr)
library(Lahman)
library(babynames)
library(kableExtra)
library(printr)
library(tidytext)
library(wordcloud)
library(dplyr)

```
### Exercise 1   


----

First, I download the dataset.

```{r prelim_1, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
library(gutenbergr)
books <- gutenberg_download(gutenberg_id = c(11, 1400),meta_fields = "title")

```

-----

#### Part a) 
##### _Question:_   


Find the 10 most common non-stop-words in Great Expectations. Create a world cloud of them.  

----

##### _Answer:_

First, I check how the titles are written. Then, I filter only _Great Expectations_, to only keep that part of the data.   
After that, I create the tibble to get a better structure.   
Next, I unnest the tokens to tidy the text.   
Then, I __anti_join__ with the __stop words__, in order to avoid them in the counting.   
Finally, I do the __word cloud__ to show the most used words.     

----    


```{r part_1a, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
books %>% distinct(title)

great_expectations <- books %>%
  filter(title == 'Great Expectations') 

great_expectations.tbl <- 
  tibble(text = great_expectations$text) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, "(?i)^chapter [0-9ivxlc]")))

tidy_great_expectations <- great_expectations.tbl %>%
  unnest_tokens(word, text)

tidy_great_expectations %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  head(10)

tidy_great_expectations %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100, colors=brewer.pal(8, "Dark2")))


```




----   

#### Part b) 
##### _Question:_

Find the 10 most common bigrams in _Great Expectations_ that do not include stop words.   


----   

##### _Answer:_

First, I get the bigrams in the text.    
Then, I separate the words into two columns.   
After, I filter those words, in both columns, that are in the stop words.    
Finally, I list the 10 most used bigrams.   

----    



```{r part_1b, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
tidy_great_expectations_bigram <- great_expectations.tbl %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

great_expectations_bigrams_sep <- tidy_great_expectations_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")

great_expectations_bigrams_filtered <- great_expectations_bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

na.omit(great_expectations_bigrams_filtered %>% 
  mutate(bigram = str_c(word1, ' ', word2)) %>%
  count(bigram, sort = TRUE)) %>%
  head(10)
```


    
----   

In this case I tidy both books first.    
Then, I join them with the sentiments list in the funcion __get_sentiments__.   
Finally, I plot the sentiments in both books.   

----   



```{r part_1c, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
tidy_books <- books %>%
  group_by(title) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
  ungroup()  %>%
  unnest_tokens(word, text)

books_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(title, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(books_sentiment, aes(index, sentiment, fill = title)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_wrap(~title, ncol = 3, scales = "free_x") + 
  theme(legend.position = "none") +
  geom_smooth(span = .15)

```


----   

### Exercise 2    


----   


First, I download the dataset.

```{r prelim_2, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
fed_rd <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-12/fed_r_d_spending.csv")

```   



#### Part a) 
##### _Question:_

Reformat this data with a separate variable called rd_budget_frac for the rd_budget as a fraction of total_outlays and two additional variables with upper and lower confidence bounds for rd_budget_frac from the linear model:
__rd_budget_frac ~ department + year__


----   

##### _Answer:_

I first create the variable __rd_budget_frac__.   
Then, I do the linear model, and after that, I calculate the confidence intervals.   
Finally, I left join them with the original dataset.   

----    



```{r part_2a, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

fed_rd.new <- fed_rd %>%
  mutate(rd_budget_frac = rd_budget/total_outlays)

fit <- lm(rd_budget_frac ~ department + year, fed_rd.new)

conf.int <- augment(fit) %>%
  mutate(fit = .fitted) %>%
  mutate(upper = .fitted + 1.96 * .se.fit) %>%
  mutate(lower = .fitted + 1.96 * .se.fit) %>%
  select(rd_budget_frac, department, year, fit, upper, lower)

fed_rd.new <- fed_rd.new %>%
  left_join(.,conf.int)
  
head(fed_rd.new)
```


---   

#### Part b) 
##### _Question:_
Create four plots showing rd_budget_frac (along with the upper and lower condence bounds from (a)) as a function of year for NASA, NSF, DHS, and DOD. (Be sure your figure looks polished.) Comment on any patterns you find.

----   

##### _Answer:_
I created a function to plot all 4 plots, since it would be repeating lines of codes. Then, I just called the function foreach department.      
    
    
----    



```{r part_2b, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
ploting <- function(department){
  
  coefs.plot=fed_rd.new[which(fed_rd.new$department==department),]
  
  ggplot(coefs.plot, aes(year, rd_budget_frac))+
    geom_point()+
    geom_line(aes(y=lower), color = "red", linetype = "dashed")+
    geom_line(aes(y=upper), color = "red", linetype = "dashed")+
    geom_smooth()
}

ploting("NASA")
ploting("NSF")
ploting("DHS")
ploting("DOD")
```

**_Comments:_**
Except for _DHS_, the rest have a _descending relation_ through the years, _DHS_ is a _flat horizontal line until 2001_, approximately.   



----

### Exercise 3    

-----

Table 16 of the le UN MigrantStockByOriginAndDestination 2015.xlsx (in the Week 5 Canvas folder) shows migration from one country to another in 2015. By eliminating the rows and columns that don't correspond to countries and then converting to a tidy dataset using gather() (i.e., a data frame with three columns: one for country of origin, one for country of destination, and the third indicating the number of people who immigrated from one country to the other), find

----   


First, I upload the file and clean the dataset.      
    
    
----    





```{r prelim_3, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
getwd()
setwd("/Users/dsarachaga/Google Drive/Rutgers/MSDS Data Wrangling/Week 5")

untidy_migrant <- readxl::read_excel("UN_MigrantStockByOriginAndDestination_2015.xlsx", sheet = "Table 16", skip = 15)

head(untidy_migrant)

untidy_migrant <- untidy_migrant %>%
  rename(to_country = X__2)

untidy_migrant <- untidy_migrant %>%
  rename(country_code = X__4)

head(untidy_migrant)
```   
    
    
    
---   

#### Part a) 
##### _Question:_   

The top five countries from which people migrate to Canada.    


----   

##### _Answer:_    


After cleaning the dataset, is easy to use _gather()_ to get the answer to this question.
 
 
 
 

```{r part_3a, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
migrate_to_canada <- untidy_migrant[which(untidy_migrant$to_country == "Canada"),]
migrate_to_canada <- migrate_to_canada %>% gather(`Afghanistan`:`Zimbabwe`, key = "from_country", value = "count")

migrate_to_canada %>%
  arrange(desc(count)) %>%
  head(5)

```    



#### Part b) 
##### _Question:_   

The top five countries to which people migrate from Canada.   


----   

##### _Answer:_    


Using _gather()_ again, I count the countries from Canada.   
I exclude the country codes from the agrupations, that is those higher or equal than 900.      
    
    
----    





```{r part_3b, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
migrate_from_canada <- untidy_migrant[which(untidy_migrant$from_country == "Canada"),]
migrate_from_canada <- migrate_from_canada %>% gather(`Afghanistan`:`Zimbabwe`, key = "to_country", value = "count")


migrate_from_canada <- untidy_migrant %>%filter(country_code < 900)
head(arrange(migrate_from_canada, desc(Canada)), n = 5)$to_country

```   




#### Part c) 
##### _Question:_   

The top 10 migration pairs of countries.   


----   

##### _Answer:_    


First, I keep only the countries and exclude regions.   
Then, I just looked for the top 10 pairs.       
    
    
----    



  

```{r part_3c, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

migration_pairs <- untidy_migrant %>%
  filter(country_code < 900) %>%
  gather(`Afghanistan`:`Zimbabwe`, key = "from_country", value = "count") %>% arrange(desc(count))

#Excluding the unnecessary columns
drop_columns <- c("Total","Other North", "Other South", "Country_code", "..1", "..3", "..5")
pairs_top_10 <- migration_pairs[ , !(names(migration_pairs) %in% drop_columns)]

pairs_top_10 %>%
  select(to_country, from_country) %>%
  head(10)

```   