---
title: "Data Wrangling Final Project"
author: "Diego Sarachaga"
date: "5/6/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, echo = FALSE}
#knitr::opts_chunk$set(include = FALSE)
library(dplyr)
library(lubridate)
library(magrittr)
library(rvest)
library(stringr)
library(tidyverse)
library(tidytext)
library(xml2)
library(lubridate)
library(rtweet) 
library(ggplot2)
library(dplyr)
library(ggmap)
library(igraph)
library(ggraph)
library(wordcloud)
library(corrplot)


set.seed(1)
# Getting API's keys
source("API-keys.R")
```

#Final Project

##Introduction

For this project the goal is to use the historical weather data from www.wunderground.com of a particular city, in this case, New York, combined with data obtained from the Twitter API.
From the Twitter API, I am going to obtain tweets from hashtags that, according to some previous web research, are related to online food delivery.   

The main analysis will be if people order food delivery more or less whether it is raining or not.   

According to the Twitter API, every tweet has date and location information (if the location was shared), so I can use the location and date to join it with the climate data.   

Moreover, each tweet may have coordinates, allowing to locate in a map where people buy.

The data sources will be:
https://www.wunderground.com/weather/us/ny/new-york-city   
https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object

The R code and report will be in my github account: 
https://github.com/dsarachaga/rutgers/tree/master/MSDS%20Data%20Wrangling/Final%20Project

##Getting the data   

###Twitter   


First, I get API tokens for the Twitter API, Wunderground API, and Google Maps API, saving all of them in separate file for security reasons.   

Next, I will begin to retrieve data from the APIs, first, from Twitter API. I will call the API twice, the first to get all tweets that contain the bigram __food delivery__, and then, one more time to get all tweets that contain the word __delivery__ or these hashtags:    

* \#onlineshopping 
* \#online 
* \#fooddelivery 
* \#delivery 
* \#grubhub 
* \#ubereats 
* \#seamless 
* \#onlinefooddelivery 
* \#onlinedelivery 
* \#onlinefood

Then, using _rbind()_, I put them in one dataframe.   
In both requests, I am filtering _retweets_ and _quotes_, because I am interested in original tweets. Also, in both requests, I am asking only for tweets that were made in __New York City__.   
Twitter limits how many days we can retrieve, 7 - 9 days from today, and also only allows us to call the API 15 times per 15 minutes. Here is where we are facing our first trouble, since we can only get around a week of history, and we wanted to get a month. Because of this, once we obtain the tweets, we are going to extract the minimum and maximum date, in order to use them with the weather API.      

```{r gettingTweets, error=TRUE, message=FALSE, warning=FALSE, include=FALSE, eval=TRUE}
twitter_token <- create_token(
  app = appname,
  consumer_key = consumer_key,
  consumer_secret = consumer_secret,
  access_token = access_token,
  access_secret = access_secret
)

identical(twitter_token, get_token())

rt.1 <- search_tweets(
  q = "\"food delivery\" -filter:retweets -filter:quote",
  n = 18000,
  geocode = lookup_coords("New York City, NY"),
  include_rts = FALSE
)

rt.2 <- search_tweets(
  q = "delivery OR #onlineshopping OR #online OR #fooddelivery OR #delivery OR #grubhub OR #ubereats OR #seamless OR #onlinefooddelivery OR #onlinedelivery OR #onlinefood -filter:retweets -filter:quote",
  n = 18000,
  geocode = lookup_coords("New York City, NY"),
  include_rts = FALSE
)

rt = rbind(rt.1, rt.2)
```

Once we retrieve Twitter's data, it looks like this:   

```{r showTweets, echo=TRUE, tidy=TRUE, error=TRUE, message=FALSE, warning=FALSE}
head(rt, 1)
```

Despite filtering by location, sometimes we get some we do not want, as we can see in this plot:   

```{r cleaningPlot, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
rt %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location, y = n)) +
  geom_col(stat="identity", fill = rgb(0, .3, .7, .75)) + 
  coord_flip() +
  labs(x = "Count",
       y = "Location",
       title = "Where the Tweets where written from - unique locations?")
```

So, to be sure, we are going to filter by location again, but this time by text, using __regular expressions__. We are going to check get only those locations that belong to this list:   

* New York
* NYC
* New York City
* Manhattan
* Bronx
* Brooklyn
* Queens

Also, we are going to round the dates by hour, to facilitate our analysis.  


```{r cleaningTweets, error=TRUE, message=FALSE, warning=FALSE, include=FALSE}
# Getting the first day from the Twitter response
if ((month(min(rt$created_at)) < 10) &(day(min(rt$created_at)) < 10)){
    first_date = paste0(toString(year(min(rt$created_at))),'0', toString(month(min(rt$created_at))),'0', toString(day(min(rt$created_at))))
  }else if (day(min(rt$created_at)) >= 10){
    first_date = paste0(toString(year(min(rt$created_at))),'0', toString(month(min(rt$created_at))), toString(day(min(rt$created_at))))
    }else{
    first_date = paste0(toString(year(min(rt$created_at))), toString(month(min(rt$created_at))), toString(day(min(rt$created_at))))
    }
  
# Getting the last day from the Twitter response
if ((month(max(rt$created_at)) < 10) &(day(max(rt$created_at)) < 10)){
  last_date = paste0(toString(year(max(rt$created_at))),'0', toString(month(max(rt$created_at))),'0', toString(day(max(rt$created_at))))
}else if (day(max(rt$created_at)) >= 10){
  last_date = paste0(toString(year(max(rt$created_at))),'0', toString(month(max(rt$created_at))), toString(day(max(rt$created_at))))
}else{
  last_date = paste0(toString(year(max(rt$created_at))), toString(month(max(rt$created_at))), toString(day(max(rt$created_at))))
}

unique(rt$location)

rt %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n))

# Filtering online tweets that have a location in NYC
rt.NYC = rt %>%
  filter(grepl('New York|NYC|New York City|Manhattan|Bronx|Brooklyn|Queens', location)) %>%
  filter(lang == "en")

#unique(rt.NYC$location)
#unique(rt.NYC$lang)
#nrow(rt.NYC)

# Rounding the hours to facilitate the analysis.
rt.NYC = rt.NYC %>%
  mutate(created_at = round_date(created_at, unit = 'hour'))

```

The total rows we obtained after the location cleansing is _`r nrow(rt.NYC)`_, and if we plot the location distribution, should get better.  

```{r afterCleaningPlot, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
rt.NYC %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location, y = n)) +
  geom_col(stat="identity", fill = rgb(0, .3, .7, .75)) + 
  coord_flip() +
  labs(x = "Count",
       y = "Location",
       title = "Where the Tweets where written from - unique locations?")
```



###Wunderground      

For this part, I am using some reuse code from homework 6, but instead of getting the weekly observations, I am getting from the dates extracted from the tweets. And, like in the tweets data, I round the dates by hour to facilitate the analysis.      

```{r gettingWeather, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE, echo = FALSE}
# Weather information
weather_url = paste0("https://api.weather.com/v1/geocode/40.77/-73.86/observations/historical.json?apiKey=",weatherAPI,"&startDate=",first_date,"&endDate=",last_date,"&units=e")
weather_raw <- weather_url %>% read_html()

df_tbl_weather <- weather_raw %>%
  html_text() %>%
  # Using jsonlite to parse it.
  jsonlite::fromJSON() %>% .[["observations"]] %>%
  select(valid_time_gmt, ## Time
         temp,           ## Temperature
         dewPt,          ## Dew Point
         rh,             ## Humidity
         wdir_cardinal,  ## Wind
         wspd,           ## Wind Speed
         gust,           ## Wind Gust
         pressure,       ## Pressure
         precip_hrly,    ## Precip.
         precip_total,   ## Precip Accum
         wx_phrase       ## Condition 
  ) %>%
  # original time is UNIX time. 
  mutate(valid_time_gmt = as_datetime(valid_time_gmt, tz = "EST"))

names(df_tbl_weather) <- c("Time", "Temperature", "Dew Point", "Humidity", "Wind", "Wind Speed", "Wind Gust", "Pressure", "Precip.", "Precip Accum", "Condition")

#Rounding the hours to facilitate the analysis.
df_tbl_weather = df_tbl_weather %>%
  mutate(Time = round_date(Time, unit = 'hour'))

head(df_tbl_weather)
```

##Joining Twitter and Wunderground dataframes   

In order to do this, we first reduce our dataframes taking only into consideration the variables that we are actually going to use. Then, we join the 2 dataframes by tweet's variable *created_at*, and weather's variable *Time*, both indicating the date the data is from.   
```{r joiningData, error = TRUE, message = FALSE, warning = FALSE, eval = TRUE, echo = FALSE}
#colnames(rt.NYC)
df_delivery = rt.NYC[, c("created_at", "text", "source", "hashtags", "lang", "country", "country_code", "geo_coords", "coords_coords", "location")]

df_weather = df_tbl_weather[, c("Time", "Temperature", "Precip.", "Precip Accum", "Condition")]

df_delivery.final <- left_join(df_delivery,df_weather, by = c("created_at" = "Time"))

df_delivery.final <- df_delivery.final[complete.cases(df_delivery.final[ , "Temperature"]),]
```


##Analyzing the data   

Now that we our data all together, let's start to see what we got.   
First, let's see how the amount of tweets are distributed along time.   


```{r plot5, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
## plot time series of tweets
ts_plot(df_delivery.final, "3 hours") +
  theme(plot.title = ggplot2::element_text(face = "bold")) +
  labs(
    x = NULL, y = NULL,
    title = "Frequency of Tweets 9 days",
    subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals"
  )

```

Next, let's see how the _Temperature_ affects the amount of tweets people make.  


```{r plot2, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
df_delivery.final %>%
  count(Temperature, sort = TRUE) %>%
  top_n(20) %>%
  as.data.frame() %>%
  arrange(desc(Temperature)) %>%
  ggplot(aes(x = Temperature, y = n)) +
  geom_bar(stat="identity", fill = rgb(0, .3, .7, .75)) + 
  labs(x = "Count",
       y = "Temperature",
       title = "What was the Temperature when the Tweets was written?")
```

From the plot we can see that when the temperature is lower, __people tend to tweet more about deliverying__. So, this is a good start, since it seems right that when it is colder, people do not want to go out and just want to stay at home.   

Now, let's see if this is also true for rain.   

```{r plot3, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
df_delivery.final %>%
  count(Precip., sort = TRUE) %>%
  top_n(20) %>%
  as.data.frame() %>%
  arrange(desc(Precip.)) %>%
  ggplot(aes(x = Precip., y = n)) +
  geom_bar(stat="identity", fill = rgb(0, .3, .7, .75)) + 
  labs(x = "Count",
       y = "Precipitation",
       title = "Was it raining when the Tweet was written?")


```

Apparently, __rain is not a factor when it comes to delivery__, opposite to what we could thought.    


In the next plot, we will see how was the overall weather condition when the tweet was written.   


```{r plot4, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
df_delivery.final %>%
  count(Condition, sort = TRUE) %>%
  mutate(Condition = reorder(Condition, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = Condition, y = n)) +
  geom_bar(fill = rgb(0, .3, .7, .75), stat = "identity") +
  coord_flip() +
  labs(x = "Count",
       y = "Condition",
       title = "What was the weather condition when the Tweet was written?")
```

This is the second assumption that seems to follow. If we take a look at the plot, __the top 5 weather conditions are not the best__, especially __cloudy__ appears as the __top 2__.    


Now, as we have our Google Maps API, we take a look at a map where were these tweets posted.  


```{r map1, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
df_delivery.final <- lat_lng(df_delivery.final)

get_map(location = c(-73.94,40.78), zoom = 12) %>% 
  ggmap() +
  geom_point(data = df_delivery.final, aes(x = lng, y = lat), col = rgb(0, .3, .7, .75), size=3, alpha=0.5)
```

Something interesting here is that when building the map, a warning appeared that almost 2/3 of the data did not have a longitude and latitude. So, despite being representative, it is not complete.   
Let's take a better look by plotting a density map and see where is the zone where they concentrate.   

```{r map2, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
#plot the density map
get_map(location = c(-73.94,40.78), zoom = 12) %>% 
  ggmap() +
  stat_density2d(
  aes(x = lng, y = lat, fill = ..level.., alpha = ..level..*2), 
  size = 2, bins = 5, data = df_delivery.final, geom = "polygon") +
  scale_fill_gradient()
```

Apparently, the south of NYC is where the most of these tweets were posted. Which could be right if the tweets were made at lunch time, because is an office zone in NYC.     

There is another interesting variable that Twitter give us, and that is the source of the tweet, meaning if it was made by a phone, with which OS, or from a computer. So, let's see what we have.   


```{r plot6, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
df_delivery.final %>% 
  group_by(source)%>% 
  summarise(Total=n()) %>% 
  arrange(desc(Total)) %>% 
  head(10) %>%
  ggplot(aes(reorder(source, Total), Total, fill = source)) + 
  geom_bar(stat="identity", fill = rgb(0, .3, .7, .75)) + 
  coord_flip() + 
  labs(title="Top Tweet Sources", x="")+
  theme_minimal()
```


There were more tweets coming from iPhone Vs Android smartphones, and Twitter web client is actually more used than Android smartphones.   



Now, we will do some text analysis. For this, we will use the tweet's text, and after doing some cleansing, we will get the most used words in all of them.   


```{r textAnalysis, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}

# First, remove http elements manually
df_delivery.final$stripped_text <- gsub("http.*","", df_delivery.final$text)
df_delivery.final$stripped_text <- gsub("https.*","", df_delivery.final$stripped_text)

# Second, remove punctuation, convert to lowercase, add id for each tweet
df_delivery.final_clean <- df_delivery.final %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)

# Third, remove stop words from your list of words
cleaned_tweet_words <- df_delivery.final_clean %>%
  anti_join(stop_words)

# Finally, plot the top 15 words
cleaned_tweet_words %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_col(position = "dodge",fill = rgb(0, .3, .7, .75))+
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Most Common words found in food delivery tweets") + 
  theme_minimal() + 
  theme(legend.position = "") 
```


From the plot, we can see that _delivery_ , _pay_ and _paper_ tops our most used words.

Just to continue doing some word analysis, let's see what happens when we do a word network searching for bigrams.   

```{r wordNetwork, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
# remove punctuation, convert to lowercase, add identity for each tweet!
delivery_tweets_paired_words <- df_delivery.final %>%
  select(stripped_text) %>%
  unnest_tokens(paired_words, stripped_text, token = "ngrams", n = 2)

delivery_tweets_separated_words <- delivery_tweets_paired_words %>%
  separate(paired_words, c("word1", "word2"), sep = " ")

delivery_tweets_filtered <- delivery_tweets_separated_words %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
delivery_words_counts <- delivery_tweets_filtered %>%
  count(word1, word2, sort = TRUE)

# plot #delivery word network
delivery_words_counts %>%
  filter(n >= 24) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
  geom_node_point(color = "dodgerblue4", size = 3) +
  geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
  labs(title = "Word Network: Tweets using delivery",
       subtitle = "Text mining twitter data ",
       x = "", y = "") +
      theme_minimal()

```

There are some interesing bigrams **special-delivery-pizza** or **paper-assingment-dissertation**. It seems that a lot of people in the middle of their assignments or dissertation prefer to order food delivery than cook themselves.   


After the tweet's text analysis, let's continue with the text analysis, but now taking a look at the hashtags. For this analysis, we will use a _wordcloud_.   

```{r hashtagAnalysis, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
hashtagFreq = tolower(as_vector(df_delivery.final$hashtags)) %>% 
  table() %>% 
  as.data.frame() %>% 
  arrange(desc(Freq)) 

names(hashtagFreq) = c("hashtag", "freq")

png("wordcloud_packages.png", width=1280,height=800)
suppressWarnings(wordcloud(hashtagFreq$hashtag,
          hashtagFreq$freq, 
          scale=c(8,.3),min.freq=10,
          max.words=Inf, 
          random.order=FALSE, 
          rot.per=.15, 
          colors=brewer.pal(6, "Dark2")))
```


This wordcloud apparently confirms that students in the middle of a dissertation or assignment tend to order food.   


Finally, we will create a correlation matrix between weather data and the amount of tweets posted. From the weather data, we will use _Temperature_, _Precipitation_, and _Weather Condition_ (previously transform into numeric).   

```{r correlationAnalysis, echo=FALSE, error=TRUE, message=FALSE, warning=FALSE}
groups = unique(df_delivery.final$Condition)
df_delivery.final$ConditionAsNum = as.numeric(factor(df_delivery.final$Condition, levels=groups))

df_delivery.cor = df_delivery.final %>% 
  group_by(Temperature,Precip., ConditionAsNum)%>% 
  summarise(Total=n()) %>% 
  arrange(desc(Total)) 

res = round(cor(df_delivery.cor),2)

corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)


```

__None of the weather variables seem to have a correlation with the amount of tweets__. Moreover, they all have a weak negative linear relationship. So, from looking at the correlation matrix, __none of our assumptions__ are seem to be true.




##Conclusion   

From what we analyze, there are some assumptions that might be true, especially when looking at the graphical analysis. However, the correlation matrix did not result as expected. There is not a denial nor an affirmation that the assumptions are false.   
Something that must be taken into account, is that we are only using last week data. For future analysis it would be better to get more historical data. Moreover, location is has a lot of NA, making it harder to get all the possible data from the city we want to analyze.
Finally, I believe that for future steps, it would be interesting to add Instagram data, since from my personal research, instragram hashtags seems to work better and people is using Instagram more than Twitter.   

